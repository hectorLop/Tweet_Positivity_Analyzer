{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd51f82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f1b26a",
   "metadata": {},
   "source": [
    "References:\n",
    "- https://medium.com/mlearning-ai/twitter-sentiment-analysis-with-deep-learning-using-bert-and-hugging-face-830005bcdbbf\n",
    "- https://www.kdnuggets.com/2022/01/finetuning-bert-tweets-classification-ft-hugging-face.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b10822",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "First, we must load the data from the CSV files. The data is from a [Kaggle dataset](https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification), so it was originally splitted into train and test. Therefore, we will concat both training and test sets to get an unique dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f61f322b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3793</th>\n",
       "      <td>3794</td>\n",
       "      <td>48746</td>\n",
       "      <td>Israel ??</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Meanwhile In A Supermarket in Israel -- People...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3794</th>\n",
       "      <td>3795</td>\n",
       "      <td>48747</td>\n",
       "      <td>Farmington, NM</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Did you panic buy a lot of non-perishable item...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3795</th>\n",
       "      <td>3796</td>\n",
       "      <td>48748</td>\n",
       "      <td>Haverford, PA</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Asst Prof of Economics @cconces was on @NBCPhi...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3796</th>\n",
       "      <td>3797</td>\n",
       "      <td>48749</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Gov need to do somethings instead of biar je r...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3797</th>\n",
       "      <td>3798</td>\n",
       "      <td>48750</td>\n",
       "      <td>Arlington, Virginia</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>I and @ForestandPaper members are committed to...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44955 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      UserName  ScreenName             Location     TweetAt  \\\n",
       "0         3799       48751               London  16-03-2020   \n",
       "1         3800       48752                   UK  16-03-2020   \n",
       "2         3801       48753            Vagabonds  16-03-2020   \n",
       "3         3802       48754                  NaN  16-03-2020   \n",
       "4         3803       48755                  NaN  16-03-2020   \n",
       "...        ...         ...                  ...         ...   \n",
       "3793      3794       48746            Israel ??  16-03-2020   \n",
       "3794      3795       48747       Farmington, NM  16-03-2020   \n",
       "3795      3796       48748        Haverford, PA  16-03-2020   \n",
       "3796      3797       48749                  NaN  16-03-2020   \n",
       "3797      3798       48750  Arlington, Virginia  16-03-2020   \n",
       "\n",
       "                                          OriginalTweet           Sentiment  \n",
       "0     @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
       "1     advice Talk to your neighbours family to excha...            Positive  \n",
       "2     Coronavirus Australia: Woolworths to give elde...            Positive  \n",
       "3     My food stock is not the only one which is emp...            Positive  \n",
       "4     Me, ready to go at supermarket during the #COV...  Extremely Negative  \n",
       "...                                                 ...                 ...  \n",
       "3793  Meanwhile In A Supermarket in Israel -- People...            Positive  \n",
       "3794  Did you panic buy a lot of non-perishable item...            Negative  \n",
       "3795  Asst Prof of Economics @cconces was on @NBCPhi...             Neutral  \n",
       "3796  Gov need to do somethings instead of biar je r...  Extremely Negative  \n",
       "3797  I and @ForestandPaper members are committed to...  Extremely Positive  \n",
       "\n",
       "[44955 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('Corona_NLP_train.csv', encoding = \"ISO-8859-1\")\n",
    "test = pd.read_csv('Corona_NLP_test.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "df = pd.concat([train, test], axis=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127242a3",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807e2065",
   "metadata": {},
   "source": [
    "Let's check the categories distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dc460da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive              0.275142\n",
       "Negative              0.243755\n",
       "Neutral               0.185341\n",
       "Extremely Positive    0.160672\n",
       "Extremely Negative    0.135091\n",
       "Name: Sentiment, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sentiment'].value_counts() / len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f0f9bf",
   "metadata": {},
   "source": [
    "The dataset is not pretty imbalanced. Considering that there are 5 categories, each category should represent a 20% of the data to be equally balanced. In this case, the less frequent category represent a 13% of the data. \n",
    "\n",
    "Let's encode the classes into integers. The categories are ordered from negative to positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46989450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3793</th>\n",
       "      <td>3794</td>\n",
       "      <td>48746</td>\n",
       "      <td>Israel ??</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Meanwhile In A Supermarket in Israel -- People...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3794</th>\n",
       "      <td>3795</td>\n",
       "      <td>48747</td>\n",
       "      <td>Farmington, NM</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Did you panic buy a lot of non-perishable item...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3795</th>\n",
       "      <td>3796</td>\n",
       "      <td>48748</td>\n",
       "      <td>Haverford, PA</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Asst Prof of Economics @cconces was on @NBCPhi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3796</th>\n",
       "      <td>3797</td>\n",
       "      <td>48749</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Gov need to do somethings instead of biar je r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3797</th>\n",
       "      <td>3798</td>\n",
       "      <td>48750</td>\n",
       "      <td>Arlington, Virginia</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>I and @ForestandPaper members are committed to...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44955 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      UserName  ScreenName             Location     TweetAt  \\\n",
       "0         3799       48751               London  16-03-2020   \n",
       "1         3800       48752                   UK  16-03-2020   \n",
       "2         3801       48753            Vagabonds  16-03-2020   \n",
       "3         3802       48754                  NaN  16-03-2020   \n",
       "4         3803       48755                  NaN  16-03-2020   \n",
       "...        ...         ...                  ...         ...   \n",
       "3793      3794       48746            Israel ??  16-03-2020   \n",
       "3794      3795       48747       Farmington, NM  16-03-2020   \n",
       "3795      3796       48748        Haverford, PA  16-03-2020   \n",
       "3796      3797       48749                  NaN  16-03-2020   \n",
       "3797      3798       48750  Arlington, Virginia  16-03-2020   \n",
       "\n",
       "                                          OriginalTweet  Sentiment  \n",
       "0     @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...          2  \n",
       "1     advice Talk to your neighbours family to excha...          3  \n",
       "2     Coronavirus Australia: Woolworths to give elde...          3  \n",
       "3     My food stock is not the only one which is emp...          3  \n",
       "4     Me, ready to go at supermarket during the #COV...          0  \n",
       "...                                                 ...        ...  \n",
       "3793  Meanwhile In A Supermarket in Israel -- People...          3  \n",
       "3794  Did you panic buy a lot of non-perishable item...          1  \n",
       "3795  Asst Prof of Economics @cconces was on @NBCPhi...          2  \n",
       "3796  Gov need to do somethings instead of biar je r...          0  \n",
       "3797  I and @ForestandPaper members are committed to...          4  \n",
       "\n",
       "[44955 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict = {\n",
    "    'Extremely Negative': 0,\n",
    "    'Negative': 1,\n",
    "    'Neutral': 2,\n",
    "    'Positive': 3,\n",
    "    'Extremely Positive': 4\n",
    "}\n",
    "\n",
    "df['Sentiment'] = df['Sentiment'].replace(label_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5e8d67",
   "metadata": {},
   "source": [
    "The maximum number of characters allowed into a tweet is 280. We are going to check the tweets lenght, in order to analyze the data contained into the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aa891f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9688/501487315.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lenght'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'OriginalTweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df['lenght'] = df['OriginalTweet'].apply(lambda x: len(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feb43e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['lenght'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fa5afa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "355"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lenght'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6908151a",
   "metadata": {},
   "source": [
    "There are several tweets with a length longer than 280, let's analyze one of these examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e65b90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"My food stock is not the only one which is empty...\\r\\r\\n\\r\\r\\nPLEASE, don't panic, THERE WILL BE ENOUGH FOOD FOR EVERYONE if you do not take more than you need. \\r\\r\\nStay calm, stay safe.\\r\\r\\n\\r\\r\\n#COVID19france #COVID_19 #COVID19 #coronavirus #confinement #Confinementotal #ConfinementGeneral https://t.co/zrlG0Z520j\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['lenght'] > 300]['OriginalTweet'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffa4972",
   "metadata": {},
   "source": [
    "As shown above, the tween contains several carriage return characters `\\r` and a URL at the end. It also contains some uppercase letters.  \n",
    "The tokenization will take care of that errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31aa754",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b79baec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df,\n",
    "                                                   df['Sentiment'].values,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=42,\n",
    "                                                   stratify=df['Sentiment'])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,\n",
    "                                                  y_train,\n",
    "                                                  test_size=0.2,\n",
    "                                                  random_state=42,\n",
    "                                                  stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ae43b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop('Sentiment', axis=1)\n",
    "X_test = X_test.drop('Sentiment', axis=1)\n",
    "X_val = X_val.drop('Sentiment', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af114679",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6685da88",
   "metadata": {},
   "source": [
    "We are going to use a pretrained `BertTokenizer`, given that we will use a BERT model to solve the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3bfebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',\n",
    "                                         do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce29e532",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoded_data_train = tokenizer.batch_encode_plus(X_train.OriginalTweet.values,\n",
    "                                                return_attention_mask = True,\n",
    "                                                pad_to_max_length = True,\n",
    "                                                max_length = 280,\n",
    "                                                return_tensors = 'pt')\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(X_val.OriginalTweet.values,\n",
    "                                                return_attention_mask = True,\n",
    "                                                pad_to_max_length = True,\n",
    "                                                max_length = 280,\n",
    "                                                return_tensors = 'pt')\n",
    "\n",
    "encoded_data_test = tokenizer.batch_encode_plus(X_test.OriginalTweet.values,\n",
    "                                                return_attention_mask = True,\n",
    "                                                pad_to_max_length = True,\n",
    "                                                max_length = 280,\n",
    "                                                return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc67c427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(y_train)\n",
    "\n",
    "# Validation set\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(y_val)\n",
    "\n",
    "# Test set\n",
    "input_ids_test = encoded_data_test['input_ids']\n",
    "attention_masks_test = encoded_data_test['attention_mask']\n",
    "labels_test = torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8f99c8",
   "metadata": {},
   "source": [
    "## Set up Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e103ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',\n",
    "                                                      num_labels = len(label_dict),\n",
    "                                                      output_attentions = False,\n",
    "                                                      output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f258e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "dataset_train = TensorDataset(input_ids_train, \n",
    "                              attention_masks_train,\n",
    "                              labels_train)\n",
    "\n",
    "# validation set\n",
    "dataset_val = TensorDataset(input_ids_val, \n",
    "                             attention_masks_val, \n",
    "                             labels_val)\n",
    "\n",
    "# test set\n",
    "dataset_test = TensorDataset(input_ids_val, \n",
    "                             attention_masks_val, \n",
    "                             labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c96f1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "#train set\n",
    "dataloader_train = DataLoader(dataset_train,\n",
    "                              sampler = RandomSampler(dataset_train),\n",
    "                              batch_size = batch_size,\n",
    "                              num_workers=4)\n",
    "\n",
    "#validation set\n",
    "dataloader_val = DataLoader(dataset_val,\n",
    "                              sampler = RandomSampler(dataset_val),\n",
    "                              batch_size = 32, #since we don't have to do backpropagation for this step\n",
    "                            num_workers=4)\n",
    "# test set\n",
    "dataloader_test = DataLoader(dataset_test,\n",
    "                              sampler = RandomSampler(dataset_test),\n",
    "                              batch_size = 32, #since we don't have to do backpropagation for this step\n",
    "                             num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83ecb123",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                 lr = 1e-5,\n",
    "                 eps = 1e-8)\n",
    "                 \n",
    "epochs = 10\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                           num_warmup_steps = 0,\n",
    "                                           num_training_steps = len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f004b48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader : DataLoader, model : BertForSequenceClassification):\n",
    "    #evaluation mode \n",
    "    model.eval()\n",
    "    \n",
    "    #tracking variables\n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in tqdm(dataloader):\n",
    "        \n",
    "        #load into GPU\n",
    "        batch = tuple(b.to('cuda') for b in batch)\n",
    "        \n",
    "        #define inputs\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2]}\n",
    "\n",
    "        #compute logits\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        #compute loss\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        #compute accuracy\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    #compute average loss\n",
    "    loss_val_avg = loss_val_total/len(dataloader) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0).argmax(1)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals\n",
    "\n",
    "def train(\n",
    "    model : BertForSequenceClassification,\n",
    "    train_loader : DataLoader,\n",
    "    val_loader : DataLoader,\n",
    "    optimizer : torch.optim.Optimizer,\n",
    "    epochs : int\n",
    "):\n",
    "    for epoch in range(1, epochs+1):\n",
    "    \n",
    "        model.train()\n",
    "        model.to('cuda')\n",
    "\n",
    "        loss_train_total = 0\n",
    "\n",
    "        predictions, true_vals = [], []\n",
    "\n",
    "        for batch in tqdm(train_loader, desc = 'Epoch {:1d}'.format(epoch)):\n",
    "\n",
    "            model.zero_grad()\n",
    "            optimizer.zero_grad() #set gradient to 0\n",
    "\n",
    "            batch = tuple(b.to('cuda') for b in batch)\n",
    "\n",
    "            inputs = {'input_ids': batch[0], \n",
    "                      'attention_mask': batch[1], \n",
    "                      'labels': batch[2]}\n",
    "\n",
    "            outputs = model(**inputs) #unpack the dict straight into inputs\n",
    "\n",
    "            loss = outputs[0]\n",
    "            loss_train_total += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            #compute accuracy\n",
    "            logits = outputs[1].detach().cpu().numpy()\n",
    "            label_ids = inputs['labels'].cpu().numpy()\n",
    "            predictions.append(logits)\n",
    "            true_vals.append(label_ids)\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            #progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item() / len(batch))})\n",
    "        \n",
    "        predictions = np.concatenate(predictions, axis=0).argmax(1)\n",
    "        true_vals = np.concatenate(true_vals, axis=0)\n",
    "        train_acc = accuracy_score(true_vals, predictions)\n",
    "\n",
    "        torch.save(model.state_dict(), f'models/BERT_ft_epoch{epoch}.model')\n",
    "\n",
    "        tqdm.write('\\n Epoch {epoch}')\n",
    "\n",
    "        loss_train_ave = loss_train_total / len(train_loader)\n",
    "        tqdm.write(f'Training loss: {loss_train_ave}')\n",
    "        tqdm.write(f'Training accuracy: {train_acc}')\n",
    "\n",
    "        val_loss, predictions, true_vals = evaluate(val_loader, model)\n",
    "        val_acc = accuracy_score(true_vals, predictions)\n",
    "        tqdm.write(f'Validation loss: {val_loss}')\n",
    "        tqdm.write(f'Val accuracy: {val_acc}')\n",
    "        \n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6482da1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fix_seed():\n",
    "    seed_val = 17\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "    \n",
    "fix_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73f84528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 900/900 [10:34<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch {epoch}\n",
      "Training loss: 1.1100105405516094\n",
      "Training loss: 0.5338013972402766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 225/225 [00:54<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7877477218045129\n",
      "Val accuracy: 0.6785763937161129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 900/900 [10:35<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch {epoch}\n",
      "Training loss: 0.6480705485741297\n",
      "Training loss: 0.7577421709360119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 225/225 [00:54<00:00,  4.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5963524097204208\n",
      "Val accuracy: 0.7789517586542472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 900/900 [10:34<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch {epoch}\n",
      "Training loss: 0.4661544943518109\n",
      "Training loss: 0.8342080567237844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 225/225 [00:54<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5509327858686447\n",
      "Val accuracy: 0.8007785346865007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 900/900 [10:33<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch {epoch}\n",
      "Training loss: 0.36283036708003946\n",
      "Training loss: 0.8774460394146884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 225/225 [00:54<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.48016989297337004\n",
      "Val accuracy: 0.832336994300014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 900/900 [10:33<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch {epoch}\n",
      "Training loss: 0.28516743247500725\n",
      "Training loss: 0.9044871572069098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 225/225 [00:54<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.4398444327712059\n",
      "Val accuracy: 0.8519393855136939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 900/900 [10:32<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch {epoch}\n",
      "Training loss: 0.2335083911485142\n",
      "Training loss: 0.9239512008619791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 225/225 [00:54<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5031424580514431\n",
      "Val accuracy: 0.8404003892673433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 900/900 [10:32<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch {epoch}\n",
      "Training loss: 0.1947571381719576\n",
      "Training loss: 0.9374717597580897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 225/225 [00:54<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5496607136891948\n",
      "Val accuracy: 0.8345613791185875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 900/900 [10:32<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch {epoch}\n",
      "Training loss: 0.16686770953755412\n",
      "Training loss: 0.9464391227277467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 225/225 [00:54<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5550175767143567\n",
      "Val accuracy: 0.8409564854719866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 900/900 [10:33<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch {epoch}\n",
      "Training loss: 0.1467093947157264\n",
      "Training loss: 0.9539466824232734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 225/225 [00:54<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5666675565308995\n",
      "Val accuracy: 0.8454052551091339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 900/900 [10:31<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch {epoch}\n",
      "Training loss: 0.13107740349860655\n",
      "Training loss: 0.9597858955197942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 225/225 [00:54<00:00,  4.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.568159257815116\n",
      "Val accuracy: 0.8466564715695816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696b4734",
   "metadata": {},
   "source": [
    "## Test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f418d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 225/225 [00:54<00:00,  4.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8466564715695816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss, preds, true_vals = evaluate(dataloader_test, model)\n",
    "acc = accuracy_score(true_vals, preds)\n",
    "\n",
    "print(f'Test Accuracy: {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ae26afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5678512147317331"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "056c9e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 883,   84,    0,    5,    0],\n",
       "       [ 196, 1415,   73,   67,    2],\n",
       "       [   2,   88, 1138,  105,    0],\n",
       "       [   3,   88,   88, 1587,  213],\n",
       "       [   2,    5,    0,   82, 1067]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(true_vals, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d4e2501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.91      0.86       972\n",
      "           1       0.84      0.81      0.82      1753\n",
      "           2       0.88      0.85      0.86      1333\n",
      "           3       0.86      0.80      0.83      1979\n",
      "           4       0.83      0.92      0.88      1156\n",
      "\n",
      "    accuracy                           0.85      7193\n",
      "   macro avg       0.84      0.86      0.85      7193\n",
      "weighted avg       0.85      0.85      0.85      7193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_vals, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "035890c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"ðŸ’¯ I've always kept logs of projects I've worked on. Imo especially important in deep learning because the latency of each experiment is large, forcing one to increase throughput (babysit and multitask multiple experiments and ideas at once). Very difficult without logs.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "df2e6819",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = tokenizer(tweet, return_tensors='pt')\n",
    "\n",
    "model = model.to('cpu')\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**info).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1beb3841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LABEL_3'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class_id = logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1fe18fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b9a14ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9697)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(logits, dim=-1)[0, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9904defc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
